---
lang: fr
lang-ref: ch.15-2
title: Méthodes d'enchâssements joints régularisées
lecturer: Alfredo Canziani et Jiachen Zhu
authors: Sai Charitha Akula
date: 12 May 2022
translation-date: 20 Jul 2022
translator: Loïck Bourdois
---

<!---
### Non-Contrastive methods

#### Non-Contrastive methods and information theory:

Most of the non-contrastive methods are based on information theory. For example: Redundancy reduction ( Barlow Twins ) and Information.  They don't require special architectures or engineering techniques.

#### VicReg:
It tries to maximize the information content of the embeddings by producing embedding variables that are decorrelated to each other. If the variables are correlated to each other, they covariate together and the information content is reduced. Thus, it prevents an informational collapse in which the variables carry redundant information. Also, this method requires a comparatively small batch size.

Two types of collapse can occur in these architectures: \\
$\textbf{Type 1}:$ Irrespective of the input, the network generates the same representation \\
$\textbf{Type 2}:$ Special collapse - Although different images have different representations, the information content is really low in each representation.       

##### Loss function:
The loss function is pushing:
1. Positive pairs closer - to be invariant to data augmentation
2. The variance of the embeddings large by pushing all of the diagonal terms of the covariance matrix large - to prevent the first kind of collapse
3. The covariance of the embeddings small by pushing all off the diagonal terms of the covariance matrix small- to prevent the second kind of collapse.

$$
\\[0.5cm]
\green{C} = \frac{1}{N} \green{H}^\top\green{H} \\[0.5 cm]

\red{L}(\boldsymbol{w},\vx,\vy) =
 \Vert \green{h_{\vx}} - \green{h_{\vy}} \Vert^2  \\[0.2cm]

 + \frac{1}{d}[ \sum_{i}^{d} ( \gamma - \,_{\vx}\green{C}_{ii}  )^+  +  ( \gamma - \, _{\vy}\green{C}_{ii} )^+ ] \\

 + \frac{1}{d}[ \sum_{i}^{d} \sum_{j \neq i}^{d}  ( _{\vx}\green{C}_{ij} )^2 + (_{\vy}\green{C}_{ij} )^2 ]

$$

---->

### Méthodes non contrastives

#### Méthodes non contrastives et théorie de l’information

La plupart des méthodes non contrastives sont basées sur la théorie de l’information. Par exemple la réduction de la redondance (Barlow Twins) et de l’information.  Elles ne nécessitent pas d’architectures spéciales ou de techniques d’ingénierie.

#### VicReg
La méthode essaie de maximiser le contenu informationnel des enchâssements en produisant des variables d'enchâssement qui sont décorrélées les unes des autres. Si les variables sont corrélées les unes aux autres, elles covarient ensemble et le contenu informationnel est réduit. Ainsi, cette méthode empêche un effondrement informationnel dans lequel les variables portent des informations redondantes. De plus, cette méthode nécessite une taille de batch relativement faible.

Deux types d'effondrement peuvent se produire dans ces architectures :  
$\textbf{Type 1}:$ Indépendamment de l'entrée, le réseau génère la même représentation.  
$\textbf{Type 2}:$ Effondrement spécial où bien que des images différentes aient des représentations différentes, le contenu en information est vraiment faible dans chaque représentation.

##### Fonction de perte
La fonction de perte :
1. rapproche les paires positives afin d’être invariant à l'augmentation de données
2. rend la variance des enchâssements grande en poussant tous les termes diagonaux de la matrice de covariance afin d’empêcher le premier type d'effondrement.
3. rend la covariance des enchâssements faible en poussant tous les termes diagonaux de la matrice de covariance afin d’éviter le second type d'effondrement.

$$
\\[0.5cm]
\green{C} = \frac{1}{N} \green{H}^\top\green{H} \\[0.5 cm]

\red{L}(\boldsymbol{w},\vx,\vy) =
 \Vert \green{h_{\vx}} - \green{h_{\vy}} \Vert^2  \\[0.2cm]

 + \frac{1}{d}[ \sum_{i}^{d} ( \gamma - \,_{\vx}\green{C}_{ii}  )^+  +  ( \gamma - \, _{\vy}\green{C}_{ii} )^+ ] \\

 + \frac{1}{d}[ \sum_{i}^{d} \sum_{j \neq i}^{d}  ( _{\vx}\green{C}_{ij} )^2 + (_{\vy}\green{C}_{ij} )^2 ]

$$


<!---
### Clustering methods

#### SwAV

This method prevents trivial solution by quantizing the embedding space. SwAV does the following:

1. Generates representations and stack the generated representations ( into $\green{H_{x}}$ and $\green{H_{y}}$ ).
2. Applies sinkhorn clustering method to each of the stacked representation to generate corresponding clustered $\green{\boldsymbol{Q}}$ matrices where each row ( $\violet{q_{\vx}}$ ) represents a one hot vector indicating the cluster the corresponding representation belongs to
3. Performs second clustering for the representations $\vh_{\vx}$ and $\vh_{\vy}$ with soft-kmeans. This step generates predictions for $\green{q_{\vx}}$ and $\green{q_{\,\vy}}$, $\violet{\tilde{q_{\vx}}}$ and $\tilde{\violet{q_{\vy}}}$, from $\vh_{\vy}$ and $\vh_{\vx}$ respectively ( Thus, called swap prediction )
4. Minimizes the loss function which is the sum of two crossentropy functions between $\green{q_{\vx}}$ and $\violet{\tilde{q_{\vx}}}$ and $\green{q_{\vy}}$ and $\violet{\tilde{q_{\vy}}}$.


<center>
<img src="{{site.baseurl}}/images/week15/15-1/1_fig7.png" height="85%" width="85%"/><br>
<b>Fig. 8</b>: SWaV
</center>

##### The Loss function:

Sinkhorn algorithm:
Sinkhorn algorithm can distribute samples to not just one cluster but to every cluster. Thus, it can help us prevent all the data clustering into a single centroid or any such nonuniform distribution. It takes in hyperparameters that allow us to deploy different levels of uniform distribution across clusters degenerating to K-means algorithm on one extreme and to the perfectly uniform distribution on the other extreme

Softargmax clustering:
Each $\green{h_{\vy}}$ is normalized. $\boldsymbol{W}\green{h_{\vy}}$ indicates similarity between $\green{h_{\vy}}$ and all other centroids. Softargmax turns the cosine similarly ( positive or negative ) into a probability.

Since this is predicting the $\green{q_{\vx}}$, we will compare the cross entropy of the prediction, $\violet{\tilde{q_{\vx}}}$, with the actual $\green{q_{\vx}}$ to measure the prediction.


$$

\green{Q_{\vx}} = \text{sinkhorn}_{\boldsymbol{W}}(\green{H_{\vx}})  \in \mathbb{R}^{ N \times K } \\\\\\[0.2 cm]

\green{Q_{\vx}} = [ \green{q_{\vx}}^1,...,\green{q_{\vx}}^N ]^\top  \\\\[0.2 cm]

\boldsymbol{W} \in \mathbb{R}^{ K \times d } : \text{dictionary} \\ \\[0.2 cm]

\violet{\tilde{q_{\vx}}} = \text{softargmax}_{\blue{\beta}}(\boldsymbol{W}\green{h}_\vy) \in \mathbb{R}^{ K}  \\ \\[0.2 cm]

\red{F}(\vx, \vy) = \red{C}(\green{q_{\vx}}, \violet{\tilde{q_{\vx}}}) + \red{C}(\green{q_{\vy}}, \violet{\tilde{q_{\vy}}})

$$


##### Interpretation of clusters:
This method partitions latent space into a few clusters automatically without labels and the hope is that these clusters will be related to the actual classes. Thus, later, we would just need a few labeled data samples to assign each cluster to the corresponding label under supervised learning.

##### Invariance to data augmentation:
Instead of pushing the pairs closer to each other, you push both the representations to be inside the same cluster.

##### Preventing trivial solution
In a trivial solution, all the representations will be the same and thus belong to the same centroid. However, with sinkhorn, different clusters have an equal number of samples, thus the representations can’t be put into one centroid, preventing a trivial solution.
---->


### Méthodes de *clustering*

#### SwAV

Cette méthode empêche la solution triviale en quantifiant l'espace d'incorporation et fonctionne de la façon suivante :

1. On génère des représentations et on les empile (pour former $\green{H_{x}}$ et $\green{H_{y}}$).
2. On applique la méthode de *clustering* de l’algorithme de Sinkhorn à chacune des représentations empilées afin de générer les matrices $\green{\boldsymbol{Q}}$ correspondantes. Chaque ligne ($\violet{q_{\vx}}$) de ces matrices représente un vecteur *one-hot* indiquant le cluster auquel appartient la représentation correspondante.
3. On effectue un deuxième clustering pour les représentations $\vh_{\vx}$ et $\vh_{\vy}$ avec un *soft*-kmeans. 
4. Cette étape génère à partir de $\vh_{\vx}$ des prédictions pour $\violet{\tilde{q_{\vx}}}$ et $\tilde{\violet{q_{\vy}}}$ et génère à partir de $\vh_{\vy}$ des prédictions pour $\green{q_{\vx}}$ et $\green{q_{\,\vy}}$. On parle alors de prédiction échangées (*swap* en anglais d’où le nom de la méthode).
5. On minimise la fonction de perte qui est la somme de deux fonctions d'entropie croisée entre $\green{q_{\vx}}$ et $\violet{\tilde{q_{\vx}}}$ et $\green{q_{\vy}}$ et $\violet{\tilde{q_{\vy}}}$.

<center>
<img src="{{site.baseurl}}/images/week15/15-1/1_fig7.png" height="85%" width="85%"/><br>
<b>Figure 1 :</b> SWaV
</center>

##### La fonction de perte	

L’algorithme de Sinkhorn :
L'algorithme de Sinkhorn permet de distribuer les échantillons non pas à un seul cluster mais à tous les clusters. Ainsi, il peut nous aider à éviter que toutes les données se regroupent en un seul centroïde ou toute autre distribution non uniforme. Il prend en compte des hyperparamètres qui nous permettent de déployer différents niveaux de distribution uniforme entre les clusters, revenant à équivaloir à l’algorithme K-means à un extrême et à la distribution uniforme à l'autre extrême.

Clustering softargmax :
Chaque $\green{h_{\vy}}$ est normalisé. L'indice $\boldsymbol{W}\green{h_{\vy}}$ indique la similarité entre $\green{h_{\vy}}$ et tous les autres centroïdes. La fonction softargmax transforme la similarité cosinus (positif ou négatif) en une probabilité.

Comme il s'agit de prédire $\green{q_{\vx}}$, nous allons comparer l'entropie croisée de la prédiction, $\violet{\tilde{q_{\vx}}$, avec le $\green{q_{\vx}}$ réel.



$$

\green{Q_{\vx}} = \text{sinkhorn}_{\boldsymbol{W}}(\green{H_{\vx}})  \in \mathbb{R}^{ N \times K } \\\\\\[0.2 cm]

\green{Q_{\vx}} = [ \green{q_{\vx}}^1,...,\green{q_{\vx}}^N ]^\top  \\\\[0.2 cm]

\boldsymbol{W} \in \mathbb{R}^{ K \times d } : \text{dictionary} \\ \\[0.2 cm]

\violet{\tilde{q_{\vx}}} = \text{softargmax}_{\blue{\beta}}(\boldsymbol{W}\green{h}_\vy) \in \mathbb{R}^{ K}  \\ \\[0.2 cm]

\red{F}(\vx, \vy) = \red{C}(\green{q_{\vx}}, \violet{\tilde{q_{\vx}}}) + \red{C}(\green{q_{\vy}}, \violet{\tilde{q_{\vy}}})

$$


##### Interprétation des clusters
Cette méthode partitionne automatiquement l'espace latent en quelques clusters sans étiquettes et notre espoir est que ces clusters seront liés aux classes réelles. Ainsi, plus tard, nous n'aurons besoin que de quelques échantillons de données étiquetées pour affecter chaque cluster à l'étiquette correspondante dans le cadre de l'apprentissage supervisé.

##### Invariance à l’augmentation de données
Au lieu de rapprocher les paires l'une de l'autre, on pousse les deux représentations à se trouver dans le même cluster.

##### Empêcher les solutions triviales
Dans une solution triviale, toutes les représentations sont identiques et appartiennent donc au même centroïde. Cependant, avec Sinkhorn, les différents clusters ont un nombre égal d'échantillons, de ce fait les représentations ne peuvent pas être placées dans un seul centroïde. Cela empêche ainsi une solution triviale.

<!---
### Other methods

The loss function for all the previous methods including contrasting methods needs a batch or pool of negative samples, thus creating problems with distributed training. However, the loss functions of these methods are local. These methods perform well but an understanding of why they don’t collapse is not yet available. Probably there's some implicit regularization happening in these networks to prevent them from converging to a trivial solution.

<center>
<img src="{{site.baseurl}}/images/week15/15-1/1_fig9.png" height="100%" width="100%"/><br>
<b>Fig. 10</b>: Other Methods
</center>

#### BYOL:
BOYL adds a predictor, predicting $\green{h_{\vy}}$ from $\green{h_{\vx}}$. The energy function ( $\red{D}$ ) is a cosine similarity between $\green{h_{\vy}}$ and predicted $\green{h_{\vy}}$. There is no term for negative samples i.e., this method only pushes positive pairs closer and has no enforcement on negative pairs. It is thought that asymmetrical architecture with extra layers makes this method work.

SimSiam is a followup version that uses a regular backbone instead of the momentum backbone

#### Dino:
The two softargmax components used have different coldness or temperature. The energy function is the cross entropy between these two, pushing them together. Even this method doesn’t enforce anything on negative samples.

#### Data2Vec:
Adds a layer norm at the end of the representation.

##### Initialization of the network:
If you initialize the network with a trivial solution, then that network will never work.  This is because if the trivial solution is already achieved, the loss function will produce a zero gradient and thus, can never escape from the trivial solution. However, in other cases, the training dynamic is adjusted in a way that they never converge in these methods.
---->


### « Autres méthodes »

La fonction de perte de toutes les méthodes précédentes, y compris les méthodes contrastives, nécessite un batch ou un pool d'échantillons négatifs, ce qui pose des problèmes pour l'entraînement distribué. Cependant, les fonctions de perte de ces méthodes sont locales. Ces méthodes sont performantes mais on ne comprend pas encore pourquoi elles ne s'effondrent pas. Il y a probablement une régularisation implicite dans ces réseaux qui les empêche de converger vers une solution triviale.

<center>
<img src="{{site.baseurl}}/images/week15/15-1/1_fig9.png" height="100%" width="100%"/><br>
<b>Figure 2 :</b> Autres méthodes
</center>

#### BYOL
BOYL ajoute un prédicteur, prédisant $\green{h_{\vy}}$ à partir de $\green{h_{\vx}}$. La fonction d'énergie ( $\red{D}$ ) est une similarité cosinus entre $\green{h_{\vy}}$ et le $\green{h_{\vy}}$ prédit. Il n'y a pas de terme pour les échantillons négatifs, c'est-à-dire que cette méthode ne fait que rapprocher les paires positives et n'a aucune action sur les paires négatives. On pense que l'architecture asymétrique avec des couches supplémentaires permet à cette méthode de fonctionner.

SimSiam est une version postérieure qui utilise un backbone normal au lieu d’un backbone avec momentum.


#### Dino
Les deux composants softargmax utilisés ont une froideur/température différente. La fonction d'énergie est l'entropie croisée entre ces deux, rapprochant ces composants. Cette méthode n'impose rien sur les échantillons négatifs.

#### Data2Vec
Ajoute une couche de norme à la fin de la représentation.

##### Initialisation du réseau
Si vous initialisez le réseau avec une solution triviale, alors ce réseau ne fonctionnera jamais. En effet, si la solution triviale est déjà atteinte, la fonction de perte produira un gradient nul et ne pourra donc jamais s'échapper de la solution triviale. Cependant, dans d'autres cas, la dynamique d'entraînement est ajustée de telle sorte qu'ils ne convergent jamais dans ces méthodes.



<!---
### Improvisations for JEMs

We can further improve these models by experimenting with data augmentation and network architecture. We don’t have a good understanding of these but they are very important. In fact, finding good augmentation may boost more performance than changing the loss function.

#### Data Augmentation

Most dominant augmentations were proposed by simCLR and improved a little bit by BYOL:
1. Random Crop (the most critical one)
2. Flip
3. Color Jitter
4. Gaussian Blur

It has been found empirically that random crop is the most critical one. It might be because the random crop is the only we can change the spatial information about the images. Flip does the same partly but is weak. Color jitter and gaussian blur change channels.

<center>
<img src="{{site.baseurl}}/images/week15/15-2/2_fig5.png" width="75%" /><br>
<b>Fig. 5</b>: Data Augmentation
</center>

##### Masking augmentation:
Recently people are moving towards mazsking augmentation instead of traditional augmentation in which we mask out most ( ~75% in the below image ) of the patches. It can replace random crop since it’s another way to remove the redundancy of the spatial information

**Issues:**
This works well only with transformer type of architecture and not with convnet. This is because masking introduces too many random artificial edges. For any transformer, the first layer is the conv layer, with kernel size equal to the patch size and thus, this never experiences artificial edges. For convnets which have sliding windows, the artificial edges can't be ignored and will result in noise.

<center>
<img src="{{site.baseurl}}/images/week15/15-2/2_fig6.png" weight="50%" /><br>
<b>Fig. 6</b>: Masked Augmentation
</center>

#### Network Architecture

##### Projector/Expander:
It is a two/three-layer feed-forward neural network and empirical results show that it is always better to add this in the network architecture.

The projector is used to project into a lower dimension and the expander is used to project into a higher dimension. A projector is used only during the pretraining and removed while performing the downstream task. This is because the projector removes a lot of information even if the output dimension of the projector and the backbone are the same.

##### Momentum Encoder:
Even without a memory bank, a momentum encoder usually helps the performance of the downstream tasks, especially with weak data augmentation.

<center>
<img src="{{site.baseurl}}/images/week15/15-2/2_fig7.png" width="50%" /><br>
<b>Fig. 7</b>: Projector/Expander
</center>
---->

### Améliorations pour les JEMs

Nous pouvons encore améliorer ces modèles en faisant des expériences sur l'augmentation des données et l'architecture du réseau. Nous n'avons pas une bonne compréhension de ces éléments, mais ils sont très importants. En fait, trouver une bonne augmentation peut améliorer les performances plus que la modification de la fonction de perte.


#### Augmentation de données

La plupart des augmentations dominantes ont été proposées par SimCLR et améliorées un peu par BYOL :
1. Recadrage aléatoire (le plus critique)
2. Retournement
3. Variation de couleur
4. Flou gaussien

Il a été constaté empiriquement que le recadrage aléatoire est le plus critique. Cela pourrait être dû au fait que le recadrage aléatoire est le seul moyen de modifier l'information spatiale des images. Le retournement fait la même chose en partie, mais il est faible. La variation de couleur et le flou gaussien changent les canaux.


<center>
<img src="{{site.baseurl}}/images/week15/15-2/2_fig5.png" width="75%" /><br>
<b>Figure 3 :</b> Augmentation de données
</center>

##### Augmentation masquée
Récemment, les chercheurs se sont tournés vers l'augmentation par masquage au lieu de l'augmentation traditionnelle dans laquelle nous masquons la plupart (~75% dans l'image ci-dessous) des patchs. Elle peut remplacer le recadrage aléatoire car c'est une autre façon de supprimer la redondance de l'information spatiale.

**Problèmes :**
Cela ne fonctionne bien qu'avec une architecture de type *transformer* et non avec de type ConvNet. En effet, le masquage introduit trop de bords artificiels aléatoires. Pour n'importe quel *transformer*, la première couche est la couche ConvNet, avec une taille de noyau égale à la taille du patch, ce qui fait qu'il n'y a jamais de bords artificiels. Pour les ConvNets qui ont des fenêtres glissantes, les bords artificiels ne peuvent pas être ignorés et se traduiront par du bruit.


<center>
<img src="{{site.baseurl}}/images/week15/15-2/2_fig6.png" weight="50%" /><br>
<b>Figure 4 :</b> Augmentation par masquage
</center>

##### Projecteur/Expandeur
Il s'agit d'un réseau neuronal *feed-forward* à deux/trois couches et les résultats empiriques montrent qu'il est toujours préférable de l'ajouter dans l'architecture du réseau.

Le projecteur est utilisé pour projeter dans une dimension inférieure et l’extenseur est utilisé pour projeter dans une dimension supérieure. Le projecteur n'est utilisé que pendant le pré-entraînement et est supprimé pendant l'exécution de la tâche en aval. En effet, le projecteur supprime beaucoup d'informations même si la dimension de sortie du projecteur et du *backbone* est la même.

##### Encodeur avec momentum
Même sans banque mémoire, un encodeur avec momentum aide généralement à la performance des tâches en aval, surtout avec une faible augmentation de données.

<center>
<img src="{{site.baseurl}}/images/week15/15-2/2_fig7.png" width="50%" /><br>
<b> Figure 5 :</b> Projecteur/Extenseur
</center>
