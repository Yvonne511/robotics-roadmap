---
lang: fr
lang-ref: ch.15
title: Semaine 15
translation-date: 31 July 2022
translator: Loïck Bourdois
---


<!--
## Lecture part A

As pointed out already, we can broadly classify Energy Based Models into generative or joint-embedding based on architectures and into contrastive or regularised & architectural based on training methods.

In this section, we discussed Visual Representation Learning, focused on self-supervised visual representation learning. This can be classified into Generative models, Pretext Tasks and Joint Embedding methods. In generative models, you train the model to reconstruct the original image from the noisy image. In pretext tasks, you train the model to figure out a smart way to generate pseudo labels. Joint Embedding methods try to make their backbone network robust to certain distortions and are invariant to data augmentation. JEM training methods can be classified into four types: contrastive methods, non-contrastive methods, clustering methods and Other methods. He concluded the lecture by discussing contrastive methods which push positive pairs closer and negative pairs away. 
-->


## Cours magistral partie A

Dans cette section, nous abordons l'apprentissage de représentations visuelles en nous concentrant sur l'apprentissage autosupervisé. Les méthodes applicables peuvent être classées en modèles génératifs, tâches de prétexte et méthodes d’enchâssements joints. Dans les modèles génératifs, on entraîne le modèle à reconstruire l'image originale à partir de l'image bruitée. Dans les tâches de prétextes, on entraîne le modèle à trouver un moyen intelligent de générer des pseudo-étiquettes. Les méthodes d’enchâssements joints tentent de rendre leur *backbone* robuste à certaines distorsions et invariant à l'augmentation des données. Les méthodes d'entraînement des JEMs peuvent être classées en quatre types : méthodes contrastives, méthodes non-contrastives, méthodes de *clustering* et les « autres méthodes ». Nous concluons en discutant des méthodes contrastives qui rapprochent les paires positives et éloignent les paires négatives. 



<!--
## Lecture part B

In this section, we discussed non-contrastive methods which are based on information theory and don’t require special architectures or engineering techniques. Then, he went on to discuss clustering methods which prevent trivial solution by quantizing the embedding space. Finally, he discussed "Other" methods which are local and don't create problem with distributed training unlike previous methods. He concluded the lecture by suggesting various improvisations for JEMs w.r.t Data augmentation and network architecture. 
-->


## Cours magistral partie B

Dans cette section, nous abordons les méthodes non-contrastives qui sont basées sur la théorie de l'information et ne nécessitent pas d'architectures ou de techniques d'ingénierie particulières. Ensuite, nous voyons les méthodes de *clustering* qui empêchent une solution triviale en quantifiant l'espace d’enchâssement. Enfin, nous parlons d’« autres méthodes » qui sont locales et ne créent pas de problème pour l’entraînement distribué contrairement aux méthodes précédentes. Nous concluons en suggérant diverses améliorations pour les JEMs par rapport à l’augmentation de données et l’architecture des réseaux.
